---
title: "STA2005S ED Assignment"
author: "Manelisi Luthuli (LTHMAN009) and Zenande Mangcipu (MNGZEN008)"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---


# Objectives, Sources of Variation, and Model Used

The primary objective of this experiment is to compare the efficiency of various sorting algorithms by measuring the time they take to sort randomly generated data sets within different sized arrays. This will allow for an understanding of how each algorithm performs under different conditions, specifically when working with varying data set sizes. The performance of the algorithms will be analysed, and the execution time captured.

Several sources of variation are considered in this experiment. The treatment factor is the different sorting algorithms with 3 levels, namely Bubblesort, Heapsort, and Countsort. The experimental units are the arrays containing randomly generated data sets. They will be sorted by each algorithm. These arrays will vary in size to simulate different conditions and ensure that the performance of the algorithms is not dependent on the specific characteristics of one particular array.

The main blocking factor in this experiment is the size of the arrays. Array size has a direct impact on the time complexity and performance of each sorting algorithm, and thus needs to be controlled. The levels of the blocking factor are defined as 50,000, 100,000, 200,000, and 400,000 sized arrays. By accounting for these sources of variation, the experiment aims to provide a fair and comprehensive comparison of the efficiency of the selected sorting algorithms.

The experiment will use a randomized block design to compare and analyse the results.

# Randomization Procedure, Data Collection/Creation, and How the Experiment Was Run

A "class" is a program that contains methods that can be used in other programs (classes). The experiment was run in the `Data` class in Java, but helper classes such as `ArrayGeneration`, `RandomArrayIndexes`, and `Shuffler` provided essential tools that assisted in carrying out the experiment. As a result of blocking according to the size of the arrays, 9 arrays were created for each block. Therefore, there were 9 arrays of size 50,000, 9 arrays of size 100,000, 9 arrays of size 200,000, and 9 arrays of size 400,000. This was to ensure repetition, as each algorithm was to act on 3 arrays of the same size within each block. In total, there were 36 arrays which were the experimental units.

The `ArrayGeneration` class was responsible for creating the arrays using two true random number generators: `/dev/urandom` and the Random.org Application Interface. The method `fetchRandomNumbersFromURandom` generated random data directly from `/dev/urandom`, a special file in Unix-like operating systems. This file provided access to entropy collected from hardware-based events like keyboard timings, mouse movements, or disk I/O. These unpredictable physical phenomena ensured the randomness of the numbers generated and stored within the arrays.

Furthermore, the Random.org Application Interface, on the other hand, was another source of true randomness that was used in the experiment. It generated random numbers by measuring atmospheric noiseâ€”specifically, variations in radio signals caused by unpredictable atmospheric conditions. The `RandomArrayIndexes` class used the Random.org Application Interface for each block to generate a unique (with no repetition of numbers) random sequence of indexes from 0 to 8. The `Shuffler` class then shuffled the initial sequence of the indexes of the 9 arrays within each block to new positions that corresponded to the random sequence that was generated by the `RandomArrayIndexes` class. This ensured that the order of the arrays was randomized for each experimental block. The shuffling process utilized the method `shuffleArrayList`, which rearranged the arrays based on the random indexes fetched from the Random.org Application Interface.

After the above step, the `RandomArrayIndexes` class was utilized again to generate a sequence of unique random indexes from 0 to 35, corresponding to all the experimental units. These were then stored as values in an array. We then looped through the array that contained the sequence of unique random indexes from 0 to 35, and each index value was used to select a specific array (experimental unit). A sorting algorithm, which is the treatment, was then assigned based on the index of the array. This treatment was determined by the function `getSortingAlgorithm(index)` in the `Data` class, which mapped different indexes to specific sorting algorithms. For instance, indexes that were multiples of 3 were assigned Bubblesort, indexes that were one less than the multiples of 3 were assigned Heapsort, and the remaining indexes were assigned Quicksort. This approach ensured that treatments were distributed in a randomized manner rather than being sequentially applied to the arrays.

Furthermore, the index also dictated the order in which the arrays were processed. Rather than always processing the arrays in the same order (e.g., first sorting out all the 50,000-sized arrays, then the 100,000-sized ones, and so on), the index determined randomly which array was sorted next. Therefore, the code ensured that the arrays of various sizes were processed in a randomized order controlled by the randomized index.

- If the index fell between 0 and 8, it selected an array from 50,000.
- If the index fell between 9 and 17, it selected an array from 100,000.
- If the index fell between 18 and 26, it selected an array from 200,000.
- If the index fell between 26 and 35, it selected an array from 400,000.

This entire process ensured that when the experiment was run, the experimental units with their assigned treatments were processed in a non-sequential, randomized order from the Random.org Application Interface, preventing any potential bias from arising due to the sequence in which arrays of different sizes were sorted.

After each treatment (sorting algorithm) was applied to an array, the time taken to sort the array was recorded in an Excel sheet.

# Difficulties Encountered During the Experiment

It was challenging to find true random generators and not pseudo-random generators. Also, there were cost constraints that arose on some of the generators. To work around these, we made use of `/dev/urandom`. We also utilized Random.org's free trial and ensured that we did not exceed the request limit for true random numbers. Additionally, coding the experiment in Java was also quite challenging.


# Randomized Block Design Model

In a Randomized Block Design (RBD), the model equation is:

$$
Y_{ij} = \mu + \tau_i + \beta_j + \epsilon_{ij}
$$

where:

- \( Y_{ij} \) is the observed response for the \(i\)-th treatment in the \(j\)-th block.
- \( \mu \) is the overall mean of the response.
- \( \tau_i \) is the effect of the \(i\)-th treatment.
- \( \beta_j \) is the effect of the \(j\)-th block.
- \( \epsilon_{ij} \) is the random error associated with the \(i\)-th treatment in the \(j\)-th block.

### Explanation

1. **Overall Mean (\( \mu \))**: Represents the average response across all treatments and blocks.
2. **Treatment Effect (\( \tau_i \))**: Measures the deviation of the \(i\)-th treatment from the overall mean.
3. **Block Effect (\( \beta_j \))**: Accounts for the variability among blocks.
4. **Error Term (\( \epsilon_{ij} \))**: Represents the random variation not explained by the treatment or block effects.

The error term \( \epsilon_{ij} \) is assumed to be normally distributed with mean 0 and constant variance \( \sigma^2 \). Mathematically, this is expressed as:

$$
\epsilon_{ij} \sim \text{N}(0, \sigma^2)
$$

where:

- \( \text{N}(0, \sigma^2) \) denotes the normal distribution with mean 0 and variance \( \sigma^2 \).
- \( \sigma^2 \) is the constant variance of the error term.

### Analysis

```{r analysis, echo=FALSE, warning=FALSE, message=FALSE}

# Install the 'readxl' package if it's not already installed

if (!requireNamespace("readxl", quietly = TRUE)) {
  install.packages("readxl")
}

# Load necessary packages
library(readxl)
library(knitr)


# Read in data from Experimental Design
Data <- read_excel("ExperimentalDesign.xlsx")
```

We will now model our data using RBD and perform ANOVA tests.

```{r B, echo=FALSE, warning=FALSE, message=FALSE}
# Random Block Design Model, ANOVA
m1 <- aov(`Time(Miliseconds)` ~ Array_Size + Sorting_Algorithm, data = Data)

# Display ANOVA table with degrees of freedom
anova_table <- summary(m1)[[1]]
anova_table_df <- data.frame(
  Term = rownames(anova_table),
  Df = anova_table$Df,
  Sum_Sq = anova_table$`Sum Sq`,
  Mean_Sq = anova_table$`Mean Sq`,
  F_value = anova_table$`F value`,
  Pr_F = anova_table$`Pr(>F)`
)
kable(anova_table_df, caption = "Detailed ANOVA Table for Random Block Design Model")
```
The ANOVA results show two significant factors:  
Array Size: $F(3, 30) = 5.005$, $p = 0.006$, indicating that different array sizes significantly impact the sorting time.  
Sorting Algorithm: $F(2, 30) = 10.567$, $p = 0.000336$, showing a statistically significant difference between the algorithms.  
Both the size of the array and the algorithm used to sort it have a statistically significant impact on sorting time. This suggests that as array sizes increase, some algorithms may become less efficient, and there are clear performance differences between sorting algorithms.
```{r A, echo=FALSE, warning=FALSE, message=FALSE}
# Generate the means table
means_table <- model.tables(m1, "means")

algorithm_means <- means_table$tables$Sorting_Algorithm

# Extract the means for the array sizes
arraysize_means <- means_table$tables$Array_Size

# Display the algorithms means table
kable(algorithm_means, caption = "Table 1: Means of the Sorting Algorithms")

# Display the array sizes means table
kable(arraysize_means, caption = "Table 2: Means for the Different Array Sizes")
```
Sorting Algorithms: Bubble Sort has a much higher mean sorting time (62,806 ms), while Heap Sort and Quick Sort are significantly faster (20 ms and 15 ms, respectively).

Array Sizes: Larger arrays (e.g., 400,000 elements) take much longer to sort compared to smaller ones (e.g., 50,000 elements).

Bubble Sort is consistently the slowest algorithm, while Heap Sort and Quick Sort are nearly identical in performance, with very small differences in sorting time.
Sorting times increase significantly as array size grows, reflecting the expected increase in time complexity for sorting large datasets.

```{r C, echo=FALSE, warning=FALSE, message=FALSE}
# Set up for plotting multiple plots per page
par(mfrow = c(2, 2))  # 2 rows and 2 columns of plots

# Boxplot of all the algorithms
boxplot(`Time(Miliseconds)` ~ Sorting_Algorithm, data = Data, las = 1,
        ylab = "Time(Miliseconds)", xlab = "Sorting Algorithm", cex.axis = 0.7)

# Remove BubbleSort from the data
filtered_data <- subset(Data, Sorting_Algorithm != "BubbleSort")

# Create the boxplot with the remaining sorting algorithms without BubbleSort
boxplot(`Time(Miliseconds)` ~ Sorting_Algorithm, data = filtered_data, las = 1,
        ylab = "Time(Miliseconds)", xlab = "Sorting Algorithm")
```
The box plots clearly show that Bubble Sort has much higher sorting times, with greater variability compared to Heap Sort and Quick Sort.
Both Heap Sort and Quick Sort have lower, similar sorting times with very little variation.

```{r E, echo=FALSE, warning=FALSE, message=FALSE}
# Compare means using Bonferroni
pairwise_result <- pairwise.t.test(Data$`Time(Miliseconds)`, Data$Sorting_Algorithm, p.adjust.method = "bonferroni")

# Convert pairwise comparison result to a data frame for better formatting
pairwise_df <- as.data.frame(pairwise_result$p.value)
kable(pairwise_df, caption = "Pairwise Comparison Results with Bonferroni Adjustment at 5% signifance level")
```

The Bonferroni-adjusted pairwise comparisons indicate significant differences between Bubble Sort and both Heap Sort and Quick Sort (p = 0.0052 for both).
However, there is no significant difference between Heap Sort and Quick Sort (p = 1.000).

Bubble Sort is significantly slower than both Heap Sort and Quick Sort.
Heap Sort and Quick Sort perform similarly, with no statistically significant difference in their efficiency.

```{r F, echo=FALSE, warning=FALSE, message=FALSE}
# Create boxplot of the blocks
boxplot(`Time(Miliseconds)` ~ Array_Size, data = Data, las = 1,
        ylab = "Time(Miliseconds)", xlab = "Array Size", cex.axis=0.7)
```
The differences between the blocks is very high. Therefore the assumption of equal variances is not met for our model, and conclusions
could be wrong.
```{r K, echo=FALSE, warning=FALSE, message=FALSE}
# Reset plotting layout to default
par(mfrow = c(1, 1))
```

```{r D, echo=FALSE, warning=FALSE, message=FALSE}
# Interaction plot containing all blocks
interaction.plot(Data$Array_Size, Data$Sorting_Algorithm, Data$`Time(Miliseconds)`,
                 col = c("red", "blue", "green"),   # Colors for each treatment level
                 lty = 1,                           # Line type
                 type = "b",                        # Line and points
                 pch = c(19, 17, 15),               # Point shapes for each treatment
                 xlab = "Array_Size", ylab = "Time(Miliseconds)", 
                 main = "Interaction Plot: Array Sizes and Sorting Algorithms", cex.axis=0.7)

# Interaction plot containing all blocks, not containing BubbleSort
interaction.plot(filtered_data$Array_Size, filtered_data$Sorting_Algorithm, filtered_data$`Time(Miliseconds)`,
                 col = c("red", "blue", "green"),   # Colors for each treatment level
                 lty = 1,                           # Line type
                 type = "b",                        # Line and points
                 pch = c(19, 17, 15),               # Point shapes for each treatment
                 xlab = "Array_Size", ylab = "Time(Miliseconds)", 
                 main = "Interaction Plot: Array Sizes and Sorting Algorithms", cex.axis=0.7)
```
The lines for HeapSort and QuickSort appear to be relatively parallel across different array sizes, indicating no significant interaction between these algorithms and array sizes.
However, BubbleSort shows a much steeper increase in sorting time as array size increases, suggesting that while there is a strong main effect of array size and sorting algorithm, there may not be a clear interaction between blocks (array sizes) and treatments (sorting algorithms) overall.
Thus, blocks and treatments do not show a strong interaction, but the performance differences between algorithms become more pronounced with larger arrays.
Therefore the assumption of no interactions for the model on our data has no evidence against it.

```{r P, echo=FALSE, warning=FALSE, message=FALSE}
# Perform Shapiro-Wilk test on residuals
shapiro_result <- shapiro.test(resid(m1))

# Convert Shapiro-Wilk test result to a data frame for better formatting
shapiro_df <- data.frame(
  Statistic = shapiro_result$statistic,
  p_value = shapiro_result$p.value
)
kable(shapiro_df, caption = "Shapiro-Wilk Test for Normality of Residuals")
```
Since the p-value is less than significance level of 0.001, we reject the null hypothesis of the Shapiro-Wilk test, which assumes that the residuals follow a normal distribution. This suggests that the residuals of the model are not normally distributed.
Thus this means that our error term of RBD assumption fails.

```{r X, echo=FALSE, warning=FALSE, message=FALSE}
# Model Diagnostics
par(mfrow = c(2, 3))

plot(m1)

hist(resid(m1), main = "", las = 1, breaks = 10)
```
Residuals vs. Fitted Plot:
The plot shows potential patterns, suggesting that the assumption of homoscedasticity might be violated. This means that the variance of residuals could be changing with different fitted values, which could lead to unreliable results from the model.

Q-Q Plot:
The Q-Q plot compares the distribution of the residuals to a theoretical normal distribution. Points should lie along the 45-degree line if the residuals are normally distributed.
In this case, the points deviate from the line, especially in the tails, indicating that the residuals are not normally distributed. This confirms the result from the Shapiro-Wilk test (W = 0.85387, p = 0.0002339), which also suggested non-normality of residuals.

Scale-Location Plot:
The Scale-Location plot (also called a Spread-Location plot) shows whether the residuals are spread equally across all levels of the fitted values. The plot should display a horizontal line with evenly scattered points if the variance of the residuals is constant.
In this case, the plot shows some trend, indicating that the residuals are not evenly spread. This suggests heteroscedasticity, meaning that the residuals' variance changes with fitted values.


The diagnostic plots and the Shapiro-Wilk test indicate that the residuals are not normally distributed.
There is evidence of heteroscedasticity, as the residuals do not have constant variance.

# Conclusion

Violations of model assumptions (normality and homoscedasticity) suggest that the conclusions drawn from the ANOVA results in the RBD model may not be fully reliable. It may be necessary to explore transformations of the data or use a non-parametric approach to address these issues.


# APPENDIX

```{r Z, echo=FALSE, warning=FALSE, message=FALSE}
print(Data)
```


The below is a link to code that runs the experiment, it can be found in the "src" folder. The R markdown code which contains that R code for the analysis can also be found here named "trial.Rmd".
The data from the experiment can also be found here named, "ExperimentalDesign.xlsx".

https://drive.google.com/drive/folders/1WJUbxh98JU9CQ8ee8euJrC_Jw6ThZQhV?usp=sharing
